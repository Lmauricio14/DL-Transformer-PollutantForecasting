# Machine Learning for Temporal Forecasting: Transformer-Based Approach to Predict Coarse Particulate Matter (PMCO) Concentrations in Mexico CitY

## Introduction

Time series forecasting is crucial in various fields, including science, technology, business, and economics. A significant public health and environmental concern is the concentration of suspended particles, or Particulate Matter (PM), especially in densely populated urban areas like Mexico City. Coarse Particulate Matter (PMCO), with micrometric sizes between 2.5 μm and 10 μm in diameter, is of special concern due to its ability to penetrate the lungs and bloodstream.

For this study, we use data provided by the Automatic Atmospheric Monitoring Network (RAMA), managed by the Ministry of the Environment of Mexico City (SEDEMA). RAMA is responsible for monitoring and recording air quality in this large metropolis. The study specifically focuses on PMCO particles.

![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Estaciones/MAP.PNG)

## Tools and Database Used

For this study, the following tools and database were used:

- **Python** and its libraries (Pandas, Matplotlib, Seaborn, TensorFlow/PyTorch) for data processing and analysis.
- **Transformers** for predictive modeling.
- **RAMA Database (2022)**: Air quality data from Mexico City provided by the Automatic Atmospheric Monitoring Network.

## Data

We selected the RAMA database for the year 2022, avoiding the atypical years of the pandemic. Monitoring stations with complete and reliable records were chosen to ensure data quality.

The selection of monitoring stations was based on statistical rigor, ensuring data completeness and minimal missing values. The chosen stations for this study are:

- **BJU (Benito Juárez), Mexico City:** Exhibiting the lowest range of PMCO concentration.
- **MER (Merced), Mexico City:** Notable for a slightly higher maximum concentration.
- **UIZ (UAM Iztapalapa), Mexico City:** Shows higher average and standard deviation of PMCO.
- **TLA (Tlalnepantla), Mexico State:** Stands out with the highest maximum concentration and standard deviation, presenting a challenging dataset with a significant amount of missing values.

The database's key statistics, including minimum, maximum, mean values, standard deviation, and missing values, highlight the variability across stations, with TLA showing notably higher PMCO concentrations. Our methodology is designed to accurately forecast PMCO behavior even in datasets with many outliers and missing values.

In preparing the data for analysis, we prioritize databases with minimal missing data. Outliers are managed using a z-scores method with a ±3 standard deviation threshold, and missing values are addressed through multiple imputation.

![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Estaciones/p001.svg)
![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Estaciones/raw%20statistics.PNG)

## Probabilistic Forecasting with Transformers

- **Deep Learning in Forecasting**: We integrate deep learning with traditional time series forecasting methods, capturing complex patterns for enhanced predictive accuracy.

- **Transformers for Sequences**: The Transformer model's Encoder-Decoder structure is ideal for time series, excelling in sequential data handling due to its attention mechanisms.

- **Attention Mechanism**: Central to the Transformer, it selectively weighs the importance of different data points in the sequence.

  ```plaintext
  Attention(Q, K, V) = Softmax((QK^T) / sqrt(d_k))V

- **Self-Attention**: This feature allows the model to process each point in context, considering the entire sequence for prediction.

  ```plaintext
  Self-Attention(X) = Softmax((XW^Q(XW^K)^T) / sqrt(d_model))XW^V

- **Encoder Complexity**: The encoder uses N blocks to deepen the network, enhancing its ability to encode context from the input sequence.

- **Multi-Head Attention**: This aspect of the model computes attention from different representation subspaces, enriching the context for predictions.

- **Decoder Output**: The decoder generates predictions step-by-step, using masked attention to prevent future data from influencing the forecast.

These elements make the Transformer a powerful tool for forecasting time series data, allowing it to learn and predict complex patterns in PMCO concentration data.

![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Protocolo/Transformers-Arquitectura.PNG)

## Objectives, Hypothesis, and Results of the Research

- **General Objective**: Evaluate the efficacy of Transformers in forecasting PMCO.
- **Hypothesis**: Transformers can capture temporal patterns in PMCO concentrations to make accurate predictions.
- **Results**: Visualization of forecasts generated by the model.

![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Forecasting/BJU.PNG)
![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Forecasting/MER.PNG)
![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Forecasting/UIZ.PNG)
![](https://github.com/Lmauricio14/Time-Series-Forecasting-for-Particles-PMCO-in-CDMX/blob/main/Forecasting/TLA.PNG)
